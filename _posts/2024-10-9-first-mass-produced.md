---
layout: post
title:  "AI视频爆发式更新｜近半年值得关注的13个闭源产品动向"
author: sal
categories: [ AI产品 ]
image: assets/images/17.jpg
tags: [featured]
---
前言 - AI视频产品近半年发展

![本文目录]({{ site.baseurl }}//assets/images/post3-1.jpg)

Sora之后，DiT架构成为视频生成产品的主流技术。这大半年里，横空出世的Luma AI、Kling成为视频生成领域最大黑马，而老牌选手Runway 也推出了实力强劲的Gen3模型。AI视频生成在画面表现力、语义理解、清晰度、可控性方面有了大幅提升，同时多家产品在音画同步、笔刷等工具上做了更新，提升了产品可控性。

这篇文章会带大家了解AI视频产品的最新能力，不同产品优势及展示各个产品实测效果。

这个系列预计分为闭源、开源两篇。以下是本文目录：

为了方便大家在PC端进行长文浏览，我用飞书搭建了这个知识库，日后撰写的文章都会保存在这里。如果你也对撰写AI文章感兴趣，欢迎后台私信联系我交流～

本文飞书链接：https://jvnkot1yjhx.feishu.cn/wiki/SLmnwRNZIiANw9kyRjoc5bMhn8g?from=from_copylink



一.先说结论
头部产品能力测评
前两个场景制作于9月初，主要针对AI视频生成最火的Runway（Gen-3）、Luma AI（Dream Machine1.5）、可灵 1.0进行了测试。

txt2vid
测试了5个不同场景的表现，Runway、Luma 各有千秋，可灵1.0在清晰度、语义理解、画面绘制上还有较大提升空间。



img2vid
Runway在光影效果中表现得不够真实，但综合下来对各场景的支持度都比较高。

Kling 1.0综合表现依然很差，但在人物特写、光影上确实有着不俗的表现，Luma则在人物特写上显得非常弱势，处理抽象材质运动时表现不凡。



Kling1.0 vs 1.5
可灵 1.5与Luma 、Runway Gen-3 的差距肉眼可见地在缩小（该案例测评于9月末）。



AI视频产品能力排名
以下均为个人多次测试的主观感受，可能会存在测试数据不足、内容维度有限的问题，欢迎大家补充



AI视频产品特点分析



AI视频生成难点小结
1.物理交互准确性还有很大进步空间

虽然视频中动作的流畅度有大幅提升，但AI视频大多还是存在运动合理性的问题。在实际测试中，有时视频中会出现动作突变，不连贯，这个问题在平面图像生视频中更明显。

这点也和现阶段视频生成时长不足有很大关系，如果提示词、首尾帧要求模型需要在短时间内处理更大幅度的运动，这种现象会更明显。



2. 训练数据的多样性还需持续提升

在我的测试中，会发现同样的场景相似能力的产品表现差异巨大。例如下文案例中，我用txt2vid中测试了一组艺术家在墙上喷绘的案例，Runway的表现极其的好，而Kling 1.5依然乏善可陈。

因工作需要，我会更关注平面风格图片生成的视频效果，由于缺乏训练数据、画面中的深度信息很难被识别等综合原因，抛开各家产品官方Demo，目前在我自己的真实测试中，所有产品在平面方向的生成效果还不够好。

通常我会先使用提示词辅助img2vid，并且在文生图时就尽量在图片中增加一些可运动的装饰性元素（花、树叶、光斑等），如果生成效果不满意我会转向Runway Gen-2笔刷工具进行精细调节来避免较大瑕疵。偶尔会出现二次元人物形象被处理成真人的情况，非常恐怖谷。

又例如在输入一张卡通狗图片时，模型无视了提示词内容，自动生成狗狗叫的“刻板印象”画面。

3. 大幅运动造成的模糊感依然没有解决。


4. 可控性依然不足。

在实际生产场景，仅靠提示词很难精细化描述我们需要对AI有更多的控制力来达到目标效果。各家在笔刷工具和运镜控制上的支持还有细微差别，还有进一步提升的空间。



一些小经验
· 如果主体物在画面中占比过小，AI倾向于产出只有镜头位移的效果，想要达到目标效果就会增加抽卡次数。所以尽量选择主体物占比大的图片

· 生视频时描述词最好是画面中有的，且易识别的内容，否则模型会自发生成新的事物，导致该内容和原本画面的不匹配，造成画面突变

· 首尾帧视频制作时，尽量保证画面中有相近、相同的元素，并保证一定相似比例，这样才能在5s的过渡动画中呈现完整、丝滑的过渡



二.热门产品介绍
Luma AI
Luma AI原本是一家3D生成公司，此前前官号推特上一直在发布3D模型相关信息，6月突然推出了AI视频生成产品 Dream Machine，在那个时间节点上，Runway、Pika迟迟未发布新模型，Sora遥不可及，Luma AI可谓是横空出世。

在海外独角兽的采访中，Luma 首席科学家表示团队主要考虑两种方案来实现4D生成：一种是图像生成3D,再将3D动画转为4D；一种则是直接生成视频内容,然后将其转为4D，而Luma认为后者更具备潜力，并且能一定程度避开3D数据集不足的问题，这是Dream Machine诞生的原因。

*4D=3D+时间，即在三维空间中随时间变化的对象或环境

Luma视频生成时会关注一致性；深度信息（例如物体和镜头的距离、物体之间的远近）；光学（光的反射折射如何在不同介质中变化）；动态的物理现象。

先看看6月我用1.0模型测试的效果，img2vid的能力非常强，能保证大幅度运动的情况下只有较少瑕疵，原图还原度也非常好。视频中每一段都是一次性输出的：

Dream Machine 1.0时期测试

Dream Machine支持通过文本指令和图像创建高质量、逼真的镜头，与Sora看得见但摸不着的体验相比，Luma AI为每个账号每月提供30次免费使用机会，但排队时间非常久。

体验地址：https://lumalabs.ai



主要功能
Dream Machine目前功能很简单，目前仅支持txt2vid 文生视频和img2vid 图生视频两种能力，除了相机运动，另外支持Extend 延长4s、循环动画、首尾帧动画能力。


产品发布不久后也公布了后续规划：将支持视频场景修改，尺寸设置、调色等功能。



txt2vid 文生视频
8月底最新发布的Dream Machine 1.5增强了提示词理解能力和视频生成能力，令我惊讶的是，Luma对视频内文字的表现也非常强，这是除Runway Gen3、海螺AI以外的产品目前无法实现的效果。


img2vid 图生视频
前阵子我在对各个产品最新模型做测评，感慨于Pika labs效果很好但过于模糊，Runway则总是生成诡异的运动… 这些产品的表现仿佛都和宣传片有很大区别。

而这正是Luma AI本次更新中最惊艳的一趴，官网公布的视频案例并没有骗人。其img2vid 生成效果在多方面远超Pika labs、Runway等产品，是目前可公开体验的AI视频生成领域的王者之一，主要体现在以下方面：

1.生成时长较长（5s），24帧/s，非常丝滑

2.运动幅度更大，能产生相机的多角度位移

3.提示词中支持增加相机、无人机控制的视角变化

4.运动过程中一致性保持的比较好，有时图片仿佛变成了3D模型

5. 分辨率高，且有效改善了运动幅度大带来的模糊感，这点是pika labs一直以来困扰我的问题



相机运动
Luma 9月初上线了类似Animatediff V2模型中的相机运动功能，我非常喜欢这个功能触发地交互，比起增加一个操作按钮，Luma通过Camera提示词触发选项。视频节奏非常轻松可爱，值得一看～




首尾帧玩法
Luma、Kling目前都支持输入开始和结束关键帧生成视频，用户可以添加Prompt提示内容变化以控制视角和主体物运动。首尾帧功能带来了很多新奇的玩法，例如：

1.多张首尾帧相连，可以连接起来成为一个更长的视频，并且多个场景的切换有种奇幻感：很多意想不到的事情被联系到了一起。


By Nick St. Pierre

2.可以通过季节、时间、环境、主体物生长变化呈现延时拍摄的效果


3.产生意想不到的衔接：

例如从微观到宏观的切换、场景之间的切换

图片

4.制作过渡动画

虽然没有达到Keynote 神奇移动的效果，现阶段用AI制作UI、PPT动画也确实有点大材小用。但能看出AI在处理首尾帧动画时会对前后两帧中的相同元素产生合理、良好的过渡效果，而只在尾帧中出现的文字元素“Design Odyssey”的动画就更多由AI自主发挥了。

未来如果有产品能增加垂类数据训练，设计师们也许能够更轻松地制作UI、PPT动画。

图片

当前，Luma也开放了Dream Machine API 供开发者调用。



Runway
作为最先推出模型的AI视频产品，Runway目前仍维持着自己的王者地位，在Luma等新产品的追击下，Runway这半年不断在模型、产品功能上快速迭代。

Runway对产品定位会更偏向影视、艺术表达。在我的测试中，Runway更擅长真实系、风景、空间视频生成；二次元场景支持不佳（通病），非常容易出现将二次元人物生成真人动画的现象。

比较特别的是，Runway支持了绿幕场景生成、视频绿幕抠像等，这将非常方便影视制作和影视后期对AIVideo进行二次处理。

这里可以看到Runway目前支持的场景和视频案例：https://runwayml.com/product/use-cases



Gen-3
7月更新的Gen-3，支持Alpha Turbo （更快速）和Alpha（更强表现力）两种模式，增强了对极度复杂提示词描述的理解，对图像运动元素的识别也有所提升。能够实现富有想象力的过渡和场景中元素的精确关键帧。

此外还支持设置图片作为动画首帧/尾帧，Gen3目前还不支持笔刷等高级调节能力。

Runway的单次生成视频时长也达到了最长10s，支持延长到40s，达到当下AI视频产品输出的最大长度。

此外，Gen-3 Alpha Turbo还支持了竖屏尺寸视频生成、加大力度发力视频到视频（风格转绘）能力，并表示即将推出更多控制能力。


特色功能
上半年，Runway在Gen2模型上推出了较多细节控制能力，并且支持精细数值调节，是当下AI视频生成产品中可控性最强的产品。

1.多笔刷控制局部运动

支持最多5个笔刷控制，包括物体运动方向、运动曲线调节。调高Ambient，笔刷绘制区域物体的运动将和周边环境产生更多关联，并加大运动幅度。

2. 相机控制
支持水平/垂直平移，水平/垂直翻转，镜头缩放/旋转。

最终生成的效果对比：

Gen3确实在清晰度上有较大提升，画面想象力更加丰富，无需复杂控制仅依靠提示词就可以得到非常好的结果。但Gen2确实更适用于对运动范围有精准控制诉求的场景。

3. Lip Sync Video
支持文本转TTS音频、音频文件换音，还有上半年大火的Lip sync video对口型能力。

图片
还可以在已经生成的视频下方选择Lip Sync 将对口型和img2vid能力结合起来，得到嘴形和视频其他部分都在动的自然效果：

图片
不论是工具栏中不断丰富的音频、视频处理能力，还是Runway Watch栏目中的优秀合作案例，都能看出Runway一直坚定得在影视制作方向发展，未来若能打通AI生成和视频剪辑能力，Runway未来将对影视制作起到至关重要的作用，成为视频领域必不可少的重要工具。



快手可灵
今年6月，被誉为国产之光的Kling凭借“让老照片动起来”、“让照片中的人拥抱”等多个热点在互联网爆火。并凭借高质量视频效果、极低的价格和良心的免费额度狠狠刷了一波好感。9月更新了1.5模型能力，让视频的画面表现力、提示词理解能力、画面清晰度直接上了一个台阶，位列视频生成产品第一梯队。

现阶段免费用户每日免费享66灵感值，1.0模型生成一次视频消耗10灵感值，1.5模型则消耗35灵感值。付费用户享有1.5模型、去水印、高清晰度视频生成、视频延长能力、大师运镜、AI生图画质增强能力。

移动端APP：快影

网页版访问：https://klingai.kuaishou.com/



传统txt2vid、img2vid以外，Kling还支持首尾帧控制、延长视频、对口型能力。

图片
txt2vid模式下支持运镜控制，普通用户可使用水平/垂直运镜、推进/拉远、垂直/水平摇镜；另有四种大师运镜很有意思，仅付费用户可用。

图片

9月中，Kling在1.0模型中增加了运动笔刷控制。

图片
此次更新的1.5模型能力在画面表现力、提示词理解能力上有大幅提升，改善了此前会将相机运动描述当作实物生成的问题。但视频中文本内容生成还是做不到，具体案例可在下文中的视频生成能力测评中查看。

在我的评测中，可灵的优势在于运动幅度较大，能产生一些惊喜效果（这点类似Luma，在img2vid中甚至优于Runway）。但是在视频清晰度、语义理解、画面表现力上，Kling 1.0都比Runway、Luma的最新模型弱很多。但1.5进步非常明显，达到了我心中付费产品的水平。



即梦AI
9月24日，字节发布了PixelDance、Seaweed两款视频AI模型，从官方Demo看，两款模型对长提示词，人物连续动作转变、多镜头组合、人物一致性的支持非常好。将有望成为国产AI头部核心竞争者。目前产品还在开放内测申请中。

网页版访问：https://jimeng.jianying.com/ai-tool/home/

内测申请：https://bit.ly/jimengai



海螺AI
MiniMax出品，出场视频演示非常惊艳，简单测试下来发现画面清晰度、画面表现力等均好过可灵1.0。在近期的Vbench 排行榜中，海螺也获得了16个维度综合评分第一名。

目前仅支持txt2vid方案。网页版访问：https://hailuoai.com/video

 

通义万相
阿里云旗下产品，上半年有多个现象级案例刷屏，例如奶牛猫跳洗澡舞、全民跳科目三（这些功能在通义千问APP中可以找到）。9月刚开放了视频生成产品的预约，一次视频生成需要10min，测试了下生成效果一般般。

网页版访问：https://tongyi.aliyun.com/

移动端下载：通义APP



Pika labs
23年Pika 一直是我心中img直出视频的王者，在二次元平面画风上的表现很好。24年2月也在业内掀起了一波浪潮，率先推出了风格选择、视频尺寸裁切、Lip Sync、延长视频等功能。但此后至今半年都不再有明显的动作。在上半年Luma、Runway的频繁更新后，Pika 清晰度差的问题愈发显著了（Topaz AI高清都很难拯救的那种），产出的视频无法满足工作场景的质量要求。

图片
Pika 工作台

10月初，Pika更新了1.5模型，拥有更逼真的动作、更清晰的分辨率。主打Pikaffects效果，支持生成爆炸、融化、膨胀、挤压、压碎、蛋糕化效果，输出非常稳定，也能符合基本物理规律。


Pika 目前仍然处于落后态势，下一步必须拿出一个更加炸裂的视频模型出来，否则很难在众多视频生成产品中脱颖而出。


Pixverse
这是一款由中国公司开发的AI视频产品，创始人是前字节跳动AI Lab视觉技术负责人，于今年1月推出了网页版，8月底更新了2.5模型。目前新用户有200免费积分，可以生成20个视频。

体验地址：https://app.pixverse.ai/



特色功能
笔刷+运动方向控制

图片
能自动识别画面中的元素

img2vid的整体能力和Runway差异不大，但Pixverse比较有特色的一个小功能是在笔刷涂抹运动区域后还可以控制运动方向。

图片

生成后的效果：

图片
但比起Luma，Pixverse生成视频的分辨率太低了，会在原图片尺寸上做压缩。



人物一致性

Pixverse的另一个特色功能是Character to video，只需要一张图片就可以生成该形象的视频，目前只支持真实风格，且视频内容是txt2vid控制的。我猜测这大概率是基于Comfyui搭建的能力打包。

图片
By：Proper

 
动漫案例制作
现阶段AI视频产品在真实系视频生成上的效果有了飞速进步，但在偏平面的漫画风格上还缺乏训练数据，画面中的可运动元素也比较难被判断，因此使用img2vid比较难产出合适的动画，动画幅度较大时很难保证前后一致性。

尝试做了一个黑白漫画画风动画，使用了Luma AI作为主力，Runway、Pixverse、SVD等工具作为辅助产出。事实证明Luma在清晰度和运动幅度上都是目前可用的AI视频工具中最好的。视频00:07光的镜头，00:15人物大幅度运球都是Luma生成的，虽有较大瑕疵，但也算意外之喜。



部分效果制作过程展示
其中有几个镜头用到了首尾帧生成视频能力。在图片生成的过程中，我通过MJ局部绘画、PS进行图片细节微调。

图片
Midjourney局部重绘 + PS简单处理

接下来我对Luma AI 、可灵1.0、Krea、Toon Crafter的首尾帧动画进行测试，在少量抽卡后选择表现最好的效果进行对比：

Luma 更接近提示词，Kling 1.0运动幅度大。

Krea的效果 着实不太行，很难保证一致性。Toon Crafter（ComfyUI）清晰度实在太差，运行时对设备要求也过高，也没有继续迭代维护，建议大家不要尝试了。

图片
另外在处理首尾帧动画时，首尾两帧之间可运动元素少、首尾间变化少，会导致可运动时长不够，最后画面中反而出现很多意料之外的动画。最终这里我用了Pixverse的笔刷+运动方向控制实现效果。

图片
SVD（ComfyUI）适合运动幅度较小的场景，可以通过后期放大、插帧得到不错的效果，就是对平面风格动画支持度一般，使用起来也有些门槛。

动漫算是视频生成非常小众的赛道，但这半年工作、个人兴趣方向上都在这个方向持续探索，希望有一天一些产品模型能够针对动漫场景训练，有机会重制这个视频。


三.其他值得关注的产品
除了致力于视频生成模型的头部产品，市场上还有一批产品致力于将类似Animatediff视频转绘制能力落地，因操作简单，无需研究复杂工作流、效果稳定受到广泛欢迎，这些产品还热衷于打造对口型、等实用功能，支持的功能都曾在国内外掀起多次大规模传播。

代表产品有：Krea AI、Viggle AI、Domo AI、Goenhance AI。



Krea AI
Krea 于23年12月上线，是目前开源社区方案产品化封装做的非常好的产品。Krea致力于将开源社区中流行的模型能力快速产品化，通过大幅降低工作流搭建成本、模型下载成本、硬件设备要求吸引用户。

从Krea 首页可以看到主打功能有实时绘画、图片生成（最新上线了Flux模型，跟进速度非常快）、视频生成、图片视频高清化四个方向，凭借快速反应市场热点持续增长。

网页版访问：https://www.krea.ai/home

官方推特：https://x.com/krea_ai



Krea 官网设计非常Framer社区的风格，值得一看

图片


实时绘画
支持拖入shapes、images，提示词辅助控制画面，还能够配合Style选择、Style权重、AI权重等维度调整，整体交互都做的非常好。你也可以通过手绘更精确地控制画面。

视频经过加速处理



视频生成
进入Generate模块，可以在Keyframes轨道插入/生成任意图片，也可以留空，通过txt控制视频生成。除了首尾帧，用户可以在任意秒数插入任意图片，自由度很高。

图片

每张图片还支持拖拽设置关键帧强度，只可惜视频生成效果比较一般，很难保证风格一致性。

图片


视频高清化
输入视频，Krea会自动根据视频内容反推prompt，几分钟后便可以得到结果，效果还行（水波纹变得更明显了），页面提供了视频前后对比（Krea的交互做的是真好）

超级慢动作
在Enhance模块，还可以通过补帧、视频变速来达到慢动作效果。比如上述动画我们觉得水波纹运动得太快了，便可以继续调整视频效果。该效果还适用于花朵绽放、液体飞溅、动作细节展示等场景。


Krea值得夸赞的除了极快的热点功能跟进能力，其交互也非常值得反复玩味。区别于其他视频生成产品采取的点击选择交互，Krea保留了视频轨道编辑器设计，用户可以任意插入、控制素材，这将为Krea未来在视频生成、视频剪辑方向的能力扩展留下很大想象空间。



ViggleAI
Viggle由一支15人团队打造，创始人是一位在多家知名公司工作过的华人AI研究员。该产品核心能力是将视频中的角色替换成其他形象，类似之前介绍过的WonderDynamics和阿里全民舞王。此前小丑模仿说唱歌手在夏季音乐节上蹦跳的视频火出了圈，并有多个视频在社媒传播形成了新的Meme Trend。

Viggle的视频工具背后依赖自家训练的3D 视频模型「JST-1」，能够根据一张角色图片生成360度角色动画，可以进行更可控的视频生成。

目前Viggle支持Discord访问和网页版访问，Discord平台已经积累了超400w用户。

网页版访问：https://www.viggle.ai/

官方推特：https://x.com/ViggleAI



Move：使图像角色移动，原始图像背景保持不变
和Mix的区别是，该模式更擅长将特定角色融入到某个动作场景中，大火的小丑视频可以看出，Viggle的视频生成稳定性、角色前后一致性非常强。用用户输入的新角色覆盖原有运动轨迹，并做到了动作、表情的高度还原。

By AIWarper


Mix：将角色图像混合到动态视频中
只需上传一个包含人物的动态视频及一张需要替换的角色照片即可，类似的Multi功能还支持上传多个角色图片，同时将多个角色融入到一个视频中，支持选择绿幕、白色背景，后期空间更大。

图片
我非常喜欢的博主enigmatic通过将蜘蛛侠角色覆盖到自行拍摄的视频中，再进行绿幕抠像、背景画面合成，最终制作出全新的动画。


By：enigmatic_e

此外，Animate功能则可以使用预设动作模板为静态角色制作动画，更全面功能在Viggle的Discord社区开放使用。



DomoAI
该产品主打风格转绘，效果十分稳定，支持的风格基本都曾在互联网上掀起浪潮，如动画风、粘土风、折纸风、像素风。此外还支持根据参考图切换视频风格（和开源模型中IPAdapter的能力非常相似）。同类产品还有Goenhance AI。

目前每个账号仅有15个免费credits，仅能生成3s视频。

网页版访问：https://domoai.app/

官方推特：https://x.com/DomoAI_


博主Framer 曾用DomoAI制作过多个AI动画视频，通过人物动作拍摄➡️ Runway 绿幕抠像 ➡️ DomoAI转绘 ➡️ MJ绘制背景图➡️ 分图层动画剪辑便能够快速做出如下效果：

在AI技术还无法精确生成目标动作的情况下，实拍转绘 + 简单后期剪辑是更易产出效果的做法。






上述产品以外， 在上一篇文章中提到的HeyGen 和 Opus Clip 是我持续关注的产品。比起迎合更多用户喜好，这两个产品从真实需求出发，在垂直领域精细打磨，并持续获取用户增长。下面来看看这两个产品近半年的一些情况吧。



Opus Clip
Opus 专注长视频转短视频场景，通过自动识别长视频中引人注目的精彩片段，将它们提取并重新排列成可传播的短视频。这是一款来自华人创业者的产品，创始人Young Z. 22年疫情期间创业做了直播相关产品创业，在尝试了多个失败的方向后，发现直播创作者的痛点是数小时的直播内容并不能被重复利用，于是Young Z. 团队提供了手动切片和AI剪辑切片能力，才逐渐形成了Opus的产品形态。

Opus是我看到的视频生成在具体需求场景落地的最好案例。23年底，产品在推出7个月后便获取了500w注册用户和1000w ARR（年度经常性收入） 。

网页版访问：https://www.opus.pro/

官方推特：https://x.com/OpusClip



Opus最新支持创作者通过Prompt输入剪辑需求，模型会对视频内容进行分析。例如输入提示：“找到我的情绪反应”并剪辑

还支持了创作者作品发布日历，能够连接到每个社交平台，同时查看您已发布、起草和计划发布的所有内容。除了为创作的短视频进行传播性评分，Opus还提供了视频数据分析后台全套创作者工具。

图片
Opus在帮助创作者获得爆发性增长后，还会持续在推特公布数据，通过案例背书吸引创作者用户形成产品自增长。

图片
HeyGen
这依然是一款来自华人创业者的产品，在短短一年多的时间里，HeyGen的 ARR 从 100 万美元增长到 3500 多万美元，这个由对口型起家的产品还在一路狂奔。

网页版访问：https://app.heygen.com/home

官方推特：https://x.com/HeyGen_Official、https://x.com/HeyGenLabs



从Heygen的功能方向来看，其并不满足于一年前对Avatar教学视频、知识新闻视频生成方向的探索。HeyGen近期支持了全身动态动作。能保持虚拟形象的面部表情和语音语调同时动态生成， 效果极其逼真。

通过URL to Video功能，你可以输入一个Amazon商品链接，无需真人录制，自动生成产品介绍视频。https://app.heygen.com/guest/url2ads

HeyGen还开始探索更逼真的沉浸式对话如何改变实时聊天体验。近期其和Zoom合作一个Beta功能，用户在生成了个人Avatar后，可以输入足够多的个人知识库，让Avatar帮助自己完成面试。

在丰富的功能以外，HeyGen也推出了类似Opus的视频Highlights剪辑功能，非常积极得寻找新场景。



四.AI视频应用场景思考
这半年来，除传统的文生视频、图生视频能力迭代外，当前的主要技术发展还围绕着通过转绘改变画风、视频内人物识别和替换方向。

图片



五.AI视频Prompt撰写参考
在测试AI视频产品模型txt2vid，img2vid能力时，我通常会基于以下结构撰写提示词

图片

你也可以参考以下内容对提示词进行丰富：

图片
当然，也可以让Claude等大模型帮助我们撰写提示词，这里是输入给语言模型的Prompt，你可以基于此进行修改：

“我正在测试Runway、Luma等视频生成产品对文本的语义理解能力和视频生成效果，现在需要你帮我写几段提示词。提示词需要满足：主体物 + 场景 + 运动内容 + 相机视角 + 氛围描述的基本内容描写，请分别给出中英文提示词内容。”

节语
今年最大的感受是AI技术进步越来越快 ，几乎每隔几天都会刷新一次新模型、产品能力的认知。

目前图像、视频生成解决的主要问题还是素材生产，各家产品还在卷数据集、模型能力，卷生成的质量、速度。比起单一素材的生产、创意表达，我会更关注可以工业化的机会，因为这里蕴含的产品化可能性更高。上半年我将大部分业余精力都投入在了ComfyUI的学习上，6月也小小实践了一下将一批AI工业化生产的内容落地上线。但素材生产的需求不是每个C端产品都有，机会可遇不可求。想打磨好一个AI功能。每个具体case其实都需要团队从模型到技术方案、生产流程、参数细节精心打磨，需要环境给予耐心，AI并不像媒体每天喊的那样颠覆颠覆，其渗透到用户的日常使用消费中，还需要各个团队沉下心来投入大量时间。

经过上半年的学习实践，我也愈发感受到AI对使用者综合能力的要求，能做出优秀作品的总是那些原本就在某个行业处于头部的人。在技术还在快速刷新的时期，AI产品的学习难度还在持续提升，随着产品能力愈来愈强，行业出现了更多对技术能力综合使用，去解决更复杂的问题的案例